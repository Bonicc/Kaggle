{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 호출하기\n",
    "- pandas 를 이용해서 데이터를 불러오고, 데이터 전처리 및 tokenize를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                     answer  \n",
       "0             i'm fine. how about yourself?  \n",
       "1       i'm pretty good. thanks for asking.  \n",
       "2         no problem. so how have you been?  \n",
       "3          i've been great. what about you?  \n",
       "4  i've been good. i'm in school right now.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dialogs.txt\",delimiter = \"\\t\", encoding = \"UTF-8\", names = [\"question\", \"answer\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(sentence):\n",
    "    # 줄임말 늘려놓기\n",
    "    sentence = re.sub(pattern = \"i'm\",    repl = \"i am\",      string = sentence)  \n",
    "    sentence = re.sub(pattern = \"you're\", repl = \"you are\",   string = sentence)\n",
    "    sentence = re.sub(pattern = \"it's\",   repl = \"it is\",     string = sentence)\n",
    "    sentence = re.sub(pattern = \"he's\",   repl = \"he is\",     string = sentence)\n",
    "    sentence = re.sub(pattern = \"she's\",  repl = \"she is\",    string = sentence)\n",
    "    \n",
    "    sentence = re.sub(pattern = \"where's\",repl = \"where is\",  string = sentence)\n",
    "    sentence = re.sub(pattern = \"what's\", repl = \"what is\",   string = sentence)\n",
    "    sentence = re.sub(pattern = \"that's\", repl = \"that is\",   string = sentence)\n",
    "    \n",
    "    sentence = re.sub(pattern = \"'ve\",    repl = \" have\",     string = sentence)\n",
    "    sentence = re.sub(pattern = \"'ll\",    repl = \" will\",     string = sentence)\n",
    "        \n",
    "    sentence = re.sub(pattern = r\"([?!,.])\",    repl = r\" \\1\",     string = sentence) # !?,. 특수문자 글자와 뗴어놓기\n",
    "    sentence = re.sub(pattern = r\"([' ']+)\",    repl = r\" \",       string = sentence) # 띄어쓰기 중복되는거 하나로\n",
    "    sentence = re.sub(pattern = r\"([^A-z1-9?!,.]+)\",    repl = r\" \",       string = sentence) # 영, 숫자 및 !?,. 빼곤 다 공백으로 처리\n",
    "    \n",
    "    sentence = sentence.strip() # 최종적으로 문자열 양옆의 공백 처리\n",
    "    sentence = \"<start> \" + sentence + \" <end>\"\n",
    "    \n",
    "    return sentence.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>pro_que</th>\n",
       "      <th>pro_ans</th>\n",
       "      <th>inputs_token</th>\n",
       "      <th>targets_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>[&lt;start&gt;, hi, ,, how, are, you, doing, ?, &lt;end&gt;]</td>\n",
       "      <td>[&lt;start&gt;, i, am, fine, ., how, about, yourself...</td>\n",
       "      <td>[1, 3, 4, 5, 6, 7, 8, 9, 2, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 10, 11, 12, 13, 5, 14, 15, 9, 2, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>[&lt;start&gt;, i, am, fine, ., how, about, yourself...</td>\n",
       "      <td>[&lt;start&gt;, i, am, pretty, good, ., thanks, for,...</td>\n",
       "      <td>[1, 10, 11, 12, 13, 5, 14, 15, 9, 2, 0, 0, 0, ...</td>\n",
       "      <td>[1, 10, 11, 16, 17, 13, 18, 19, 20, 13, 2, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>[&lt;start&gt;, i, am, pretty, good, ., thanks, for,...</td>\n",
       "      <td>[&lt;start&gt;, no, problem, ., so, how, have, you, ...</td>\n",
       "      <td>[1, 10, 11, 16, 17, 13, 18, 19, 20, 13, 2, 0, ...</td>\n",
       "      <td>[1, 21, 22, 13, 23, 5, 24, 7, 25, 9, 2, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>[&lt;start&gt;, no, problem, ., so, how, have, you, ...</td>\n",
       "      <td>[&lt;start&gt;, i, have, been, great, ., what, about...</td>\n",
       "      <td>[1, 21, 22, 13, 23, 5, 24, 7, 25, 9, 2, 0, 0, ...</td>\n",
       "      <td>[1, 10, 24, 25, 26, 13, 27, 14, 7, 9, 2, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "      <td>[&lt;start&gt;, i, have, been, great, ., what, about...</td>\n",
       "      <td>[&lt;start&gt;, i, have, been, good, ., i, am, in, s...</td>\n",
       "      <td>[1, 10, 24, 25, 26, 13, 27, 14, 7, 9, 2, 0, 0,...</td>\n",
       "      <td>[1, 10, 24, 25, 17, 13, 10, 11, 28, 29, 30, 31...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question  \\\n",
       "0               hi, how are you doing?   \n",
       "1        i'm fine. how about yourself?   \n",
       "2  i'm pretty good. thanks for asking.   \n",
       "3    no problem. so how have you been?   \n",
       "4     i've been great. what about you?   \n",
       "\n",
       "                                     answer  \\\n",
       "0             i'm fine. how about yourself?   \n",
       "1       i'm pretty good. thanks for asking.   \n",
       "2         no problem. so how have you been?   \n",
       "3          i've been great. what about you?   \n",
       "4  i've been good. i'm in school right now.   \n",
       "\n",
       "                                             pro_que  \\\n",
       "0   [<start>, hi, ,, how, are, you, doing, ?, <end>]   \n",
       "1  [<start>, i, am, fine, ., how, about, yourself...   \n",
       "2  [<start>, i, am, pretty, good, ., thanks, for,...   \n",
       "3  [<start>, no, problem, ., so, how, have, you, ...   \n",
       "4  [<start>, i, have, been, great, ., what, about...   \n",
       "\n",
       "                                             pro_ans  \\\n",
       "0  [<start>, i, am, fine, ., how, about, yourself...   \n",
       "1  [<start>, i, am, pretty, good, ., thanks, for,...   \n",
       "2  [<start>, no, problem, ., so, how, have, you, ...   \n",
       "3  [<start>, i, have, been, great, ., what, about...   \n",
       "4  [<start>, i, have, been, good, ., i, am, in, s...   \n",
       "\n",
       "                                        inputs_token  \\\n",
       "0  [1, 3, 4, 5, 6, 7, 8, 9, 2, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 10, 11, 12, 13, 5, 14, 15, 9, 2, 0, 0, 0, ...   \n",
       "2  [1, 10, 11, 16, 17, 13, 18, 19, 20, 13, 2, 0, ...   \n",
       "3  [1, 21, 22, 13, 23, 5, 24, 7, 25, 9, 2, 0, 0, ...   \n",
       "4  [1, 10, 24, 25, 26, 13, 27, 14, 7, 9, 2, 0, 0,...   \n",
       "\n",
       "                                       targets_token  \n",
       "0  [1, 10, 11, 12, 13, 5, 14, 15, 9, 2, 0, 0, 0, ...  \n",
       "1  [1, 10, 11, 16, 17, 13, 18, 19, 20, 13, 2, 0, ...  \n",
       "2  [1, 21, 22, 13, 23, 5, 24, 7, 25, 9, 2, 0, 0, ...  \n",
       "3  [1, 10, 24, 25, 26, 13, 27, 14, 7, 9, 2, 0, 0,...  \n",
       "4  [1, 10, 24, 25, 17, 13, 10, 11, 28, 29, 30, 31...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"pro_que\"] = df['question'].map(lambda x: preprocess_string(x))\n",
    "df[\"pro_ans\"] = df['answer'].map(lambda x: preprocess_string(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Tokenize and Padding\n",
    "- 데이터를 숫자로 Tokenize, 이후에 길이를 맞추기 위해 padding을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_token = {\"<padding>\" : 0, \"<start>\" : 1, \"<end>\" : 2}\n",
    "max_length = 25\n",
    "\n",
    "def tokenize(sentence, training = True):\n",
    "    tokenized_sentence = []\n",
    "    \n",
    "    if training:\n",
    "        for word in sentence:\n",
    "            try :\n",
    "                tokenized_sentence.append(word_to_token[word])\n",
    "            except:\n",
    "                word_to_token[word] = len(word_to_token)\n",
    "                tokenized_sentence.append(word_to_token[word])\n",
    "    else:\n",
    "        for word in sentence:\n",
    "            try :\n",
    "                tokenized_sentence.append(word_to_token[word])\n",
    "            except:\n",
    "                print(\"<Error!> : There is no Token for \"+word+\"! Please try again\")\n",
    "                raise NotImplementedError\n",
    "    \n",
    "    return tokenized_sentence\n",
    "\n",
    "def padding(tokenized_sentence, max_length):\n",
    "    if len(tokenized_sentence) > max_length:\n",
    "        print(\"<Error!> : max_length is small then sentence! please input bigger max_length !\")\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    while len(tokenized_sentence) < max_length:\n",
    "        tokenized_sentence.append(0)\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inputs_token'] = df['pro_que'].map(tokenize).map(lambda x: padding(x, max_length))\n",
    "df['targets_token'] = df['pro_ans'].map(tokenize).map(lambda x: padding(x, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_word = {word_to_token[i] : i for i in word_to_token}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 만들기\n",
    "- Encoder Model\n",
    "- Attention Model\n",
    "- Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 모델 정의\n",
    "- Encoder는 입력 sequence 를 받아들이고, hidden state 및 output을 생성한다.\n",
    "- 즉 채팅시에 상대방이 입력하는 입력값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 units, # encoder로 들어오는 unit의 개수\n",
    "                 vocab_size, # 임베딩전 단어의 개수\n",
    "                 embedding_units): # 임베딩 한 유닛의 개수\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = units \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_units) \n",
    "                \n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "    def __call__(self, x, hidden = None):\n",
    "        x = self.embedding(x)\n",
    "        if hidden == None:\n",
    "            hidden = tf.zeros([x.shape[0],self.enc_units])\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention 모델 정의\n",
    "- Attention은 Encoder의 output(hidden state) 들에서 주의해야할 부분을 추출해 Decoder에 입력으로써 사용된다.\n",
    "- RNN의 특징인 gradient banishing problem 을 제거하기 위한 방법\n",
    "- BahdanauAttention을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w1 = tf.keras.layers.Dense(units)\n",
    "        self.w2 = tf.keras.layers.Dense(units)\n",
    "        self.v = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def __call__(self, \n",
    "                 query, # decoder의 t-1에서의 output\n",
    "                 values): # encoder의 output들\n",
    "        \n",
    "        query = tf.expand_dims(query, 1) # query 는 decoder의 output 1개이므로, 이후에 연산을 위해 dimention 1 추가\n",
    "        score = self.v(tf.nn.tanh(self.w1(query) + self.w2(values))) # Encoder Cell 별 점수\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis = 1) # Encoder cell 별 weight(점수 기반)\n",
    "        \n",
    "        context_vector = attention_weights * values \n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1) # \n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 모델 정의\n",
    "- Decoder 는 이전의 output(output of t-1)의 값을 입력으로 다시 받는 RNN 모델\n",
    "- BahdanauAttention 사용 시에는 각 입력값에 context_vector 가 추가된 값이 들어가 Encoder의 중요 cell의 가중치를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 units, # encoder로 들어오는 unit의 개수\n",
    "                 vocab_size, # 임베딩전 단어의 개수\n",
    "                 embedding_units): # 임베딩 한 유닛의 개수\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = units \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_units) \n",
    "                \n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size) # hidden_state 에서 특정 단어로 만들어주는 layer\n",
    "                \n",
    "        self.attention = BahdanauAttention(units) \n",
    "        \n",
    "    def __call__(self, x, # 시간 t에 대한 1개의 sequence를 가진 input, shape of (batch_size, 1, embedding_units)\n",
    "                 hidden, # t-1 시간의 hidden_state, initial hidden_state 는 encoder의 마지막 hidden_state\n",
    "                 enc_output): # attention에 넣을 encoder output들(hidden_state들)\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        x = self.embedding(x) # shape of (batch_size, 1, embedding_units)\n",
    "                \n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1),x],axis = -1)\n",
    "        # context vector shape (batch_size, hidden_state) 이므로 x와 concat 하기 위해 축 1개 추가\n",
    "        \n",
    "        output, state = self.gru(x)        \n",
    "        output = tf.reshape(output, (-1, output.shape[2])) # fc에 넣기 위해 shape 조정\n",
    "        \n",
    "        x = self.fc(output) # shape of (batch_size, vocab_size)\n",
    "        \n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정의한 모델 확인\n",
    "- Encoder \n",
    "- Attention\n",
    "- Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "units = 1024\n",
    "vocab_size = len(word_to_token)\n",
    "embedding_units = 256\n",
    "\n",
    "inputs = pd.DataFrame(df.inputs_token.tolist(), index= df.index).values\n",
    "targets = pd.DataFrame(df.targets_token.tolist(), index= df.index).values\n",
    "\n",
    "steps = len(inputs)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_output shape :  (64, 25, 1024)\n",
      "enc_hidden shape :  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(units,vocab_size,embedding_units)\n",
    "enc_output, enc_hidden = encoder(inputs[:batch_size])\n",
    "\n",
    "print(\"enc_output shape : \", enc_output.shape)\n",
    "print(\"enc_hidden shape : \", enc_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector shape :  (64, 1024)\n",
      "attention_weights shape :  (64, 25, 1)\n"
     ]
    }
   ],
   "source": [
    "attention = BahdanauAttention(units)\n",
    "context_vector, attention_weights = attention(enc_hidden, enc_output)\n",
    "\n",
    "print(\"context_vector shape : \", context_vector.shape)\n",
    "print(\"attention_weights shape : \", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_output shape :  (64, 2442)\n",
      "dec_hidden shape :  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(units, vocab_size, embedding_units)\n",
    "dec_output, dec_hidden = decoder(targets[:batch_size,:1], enc_hidden, enc_output)\n",
    "\n",
    "\n",
    "print(\"dec_output shape : \", dec_output.shape)\n",
    "print(\"dec_hidden shape : \", dec_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function 정의 및 Train step 정의\n",
    "- Loss Function : \n",
    "- Train step :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, hypothesis):    \n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, 0))   # <padding> 일 경우에는 loss값을 얻지 않기 위한 mask\n",
    "    loss_ = loss_object(real, hypothesis)\n",
    "    \n",
    "    #mask = tf.cast(mask, dtype=loss_.dtype) # mask 와 loss 를 곱해주기 위해 type 변경\n",
    "    #loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(dataset):\n",
    "    loss = 0        \n",
    "    \n",
    "    train_input, train_target = dataset        \n",
    "    max_length = train_target.shape[1]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(train_input)\n",
    "        \n",
    "        dec_hidden = enc_hidden \n",
    "        dec_input = tf.ones([train_target.shape[0], 1])\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            prediction, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(train_target[:,t:t+1], prediction)\n",
    "            \n",
    "            dec_input = train_target[:, t:t+1]\n",
    "    \n",
    "    batch_loss = loss / max_length\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients,variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(inputs, targets):\n",
    "    random_index = np.arange(inputs.shape[0])\n",
    "    np.random.shuffle(random_index)\n",
    "    \n",
    "    return inputs[random_index], targets[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epochs 0, total loss is 201.13304138183594\n",
      "At epochs 1, total loss is 106.64305877685547\n",
      "At epochs 2, total loss is 93.79425811767578\n",
      "At epochs 3, total loss is 88.33040618896484\n",
      "At epochs 4, total loss is 82.79292297363281\n",
      "At epochs 5, total loss is 77.77149963378906\n",
      "At epochs 6, total loss is 72.59521484375\n",
      "At epochs 7, total loss is 67.55438232421875\n",
      "At epochs 8, total loss is 61.37772750854492\n",
      "At epochs 9, total loss is 55.68754196166992\n",
      "At epochs 10, total loss is 50.69232177734375\n",
      "At epochs 11, total loss is 44.8255500793457\n",
      "At epochs 12, total loss is 41.03865051269531\n",
      "At epochs 13, total loss is 67.57832336425781\n",
      "At epochs 14, total loss is 39.26240158081055\n",
      "At epochs 15, total loss is 34.52922821044922\n",
      "At epochs 16, total loss is 34.31454086303711\n",
      "At epochs 17, total loss is 29.90668296813965\n",
      "At epochs 18, total loss is 25.68297576904297\n",
      "At epochs 19, total loss is 22.920162200927734\n",
      "At epochs 20, total loss is 20.527727127075195\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for e in range(epochs+1):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for s in range(steps):\n",
    "        batched_dataset = (inputs[batch_size * s: batch_size * s+1], targets[batch_size * s: batch_size * s+1])\n",
    "        loss = train_step(batched_dataset)\n",
    "        total_loss += loss\n",
    "        \n",
    "    print(\"At epochs {}, total loss is {}\".format(e, total_loss))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with question\n",
    "- 모델 학습이 되었는지 채팅으로 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokenized_sentence):\n",
    "    sentence = []\n",
    "    for index in tokenized_sentence:\n",
    "        index = int(index)\n",
    "        sentence.append(token_to_word[index])\n",
    "    return sentence\n",
    "\n",
    "def cut_from_start_to_end(sentence):\n",
    "    answer = []\n",
    "    for word in sentence:\n",
    "        if word != '<start>' and word != \"<end>\":\n",
    "            answer.append(word)\n",
    "        elif word == \"<end>\":\n",
    "            break\n",
    "    return \" \".join(answer)\n",
    "    \n",
    "def chat(question):\n",
    "    question = preprocess_string(question)\n",
    "    question = tokenize(question,training = False)\n",
    "    question = padding(question, max_length)\n",
    "    \n",
    "    question = tf.expand_dims(question,0)\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(question)\n",
    "\n",
    "    dec_hidden = enc_hidden \n",
    "    dec_input = tf.ones([1, 1]) # <start> as input\n",
    "\n",
    "    for t in range(max_length):\n",
    "        answer.append(dec_input[0][0])\n",
    "        prediction, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "        dec_input = tf.expand_dims(tf.argmax(prediction,axis = 1),0)\n",
    "    \n",
    "    answer = detokenize(answer)\n",
    "    answer = cut_from_start_to_end(answer)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: hi, how are you doing? \n",
      "answer\t: then why does everyone lock their doors ?\n",
      "question: i'm fine. how about yourself? \n",
      "answer\t: i would wait to graduate .\n",
      "question: i'm pretty good. thanks for asking. \n",
      "answer\t: i would wait to graduate .\n",
      "question: no problem. so how have you been? \n",
      "answer\t: i thought i thought i thought i thought i thought i thought i thought i thought i thought i thought i thought i\n",
      "question: i've been great. what about you? \n",
      "answer\t: i would wait until october .\n",
      "question: i've been good. i'm in school right now. \n",
      "answer\t: i would wait until october .\n",
      "question: what school do you go to? \n",
      "answer\t: i thought i don t like it ?\n",
      "question: i go to pcc. \n",
      "answer\t: i would wait until october .\n",
      "question: do you like it there? \n",
      "answer\t: all night long we heard people are a good reason .\n",
      "question: it's okay. it's a really big campus. \n",
      "answer\t: i would buy her cd if you would buy her cd if you would buy her cd if you would buy her cd\n",
      "question: good luck with school. \n",
      "answer\t: i am glad you don t like it reheated .\n",
      "question: how's it going? \n",
      "answer\t: i wash them , and then steam them , and then steam them , and then steam them , and then steam them\n",
      "question: i'm doing well. how about you? \n",
      "answer\t: i would wait to learn ?\n",
      "question: never better, thanks. \n",
      "answer\t: i thought i thought i thought i thought i thought i thought i thought i thought i thought i thought i thought i\n",
      "question: so how have you been lately? \n",
      "answer\t: i am glad you don t complain very much .\n",
      "question: i've actually been pretty good. you? \n",
      "answer\t: i would wait until october .\n",
      "question: i'm actually in school right now. \n",
      "answer\t: i would wait to graduate .\n",
      "question: which school do you attend? \n",
      "answer\t: i am glad you don t like it reheated .\n",
      "question: i'm attending pcc right now. \n",
      "answer\t: i would wait to graduate .\n",
      "question: are you enjoying it there? \n",
      "answer\t: why did he sign it ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    question = df.question.loc[i]\n",
    "    print(\"question:\",question,\"\\nanswer\\t:\",chat(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
